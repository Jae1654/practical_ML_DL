{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2 Use Tensorflow like Numpy\n",
    "\n",
    "tf.constant(): create immutable constant tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Constant Tensor\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "t.shape: (2, 3)\n",
      "t.dtype: <dtype: 'float32'>\n",
      "t[:, 1:] \n",
      " [[2. 3.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(\"1. Constant Tensor\")\n",
    "print(t) # Show variable\n",
    "print('')\n",
    "\n",
    "print(f't.shape: {t.shape}')\n",
    "print(f't.dtype: {t.dtype}')\n",
    "print(f't[:, 1:] \\n {t[:, 1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1 Operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "tf.square(t) \n",
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elementwise multiplication\n",
    "t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.squeeze\n",
    "\n",
    "- 크기가 1인 차원(Axis)을 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1]\n",
      "  [2]\n",
      "  [3]]], shape=(1, 3, 1), dtype=int32)\n",
      "shape before squeeze (1, 3, 1)\n",
      "shape after squeeze (3,)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[[1], [2], [3]]])  # Shape: (1, 3, 1)\n",
    "print(x)  # Output: [[[1], [2], [3]]]\n",
    "print(f\"shape before squeeze {x.shape}\")\n",
    "result = tf.squeeze(x)  # Shape: (3,)\n",
    "print(f\"shape after squeeze {result.shape}\")\n",
    "print(result)  # Output: [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.tile(input, multiples)\n",
    "\n",
    "- 특정 차원(axis)을 따라 복제(반복)합니다\n",
    "\n",
    "Parameters\n",
    "\n",
    "- input: 복제할 대상 텐서.\n",
    "- multiples: 각 축(axis)에서 반복할 횟수를 나타내는 정수 리스트.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]\n",
      " [1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]], shape=(4, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2],\n",
    "                 [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "# Repeat 2 times along axis 0, 3 times along axis 1\n",
    "result = tf.tile(x, multiples=[2, 3])  # Shape: (4, 6)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Type Casting\n",
    "\n",
    "- tensorflow does not automatically cast any data types\n",
    "- any operation between other data type will raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.constant(40., dtype = tf.float64)\n",
    "t2 = tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.4 Variable\n",
    "\n",
    "- tf.constant is immutable\n",
    "- for weight update, need variable to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.,], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign\n",
    "\n",
    "- tf variable valus cannot be normally changed\n",
    "- need assign function to change variable values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1.00e+02, 4.20e+01, 1.00e-01],\n",
       "       [5.12e+02, 6.40e+02, 2.00e+02]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v) #[2., 4., 6.]\n",
    "v[0, 1].assign(42) # (0,1) element become 42\n",
    "v[:, 2].assign([0.1, 1.0]) # all rows at 2nd column become 0.1, 1.0\n",
    "\n",
    "# Assign new values for speicif elements\n",
    "v.scatter_nd_update(\n",
    "    indices = [[0,0], [1,2]], updates = [100., 200.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "**1. `tf.SparseTensor`**\n",
    "\n",
    "- **설명**: 희소 텐서를 표현. 대부분의 값이 0인 데이터를 효율적으로 저장.\n",
    "- **사용 예**: 희소 행렬(예: 원-핫 인코딩, NLP에서 단어 벡터).\n",
    "- **특징**:\n",
    "  - 좌표(`indices`), 값(`values`), 전체 모양(`dense_shape`)으로 구성.\n",
    "  - 메모리 사용량을 줄임.\n",
    "\n",
    "**2. `tf.TensorArray`**\n",
    "\n",
    "- **설명**: 가변 길이의 텐서 배열. 반복문이나 그래프 모드에서 동적으로 텐서를 저장/추출.\n",
    "- **사용 예**: RNN, 동적 계산 그래프에서 중간 계산 결과 저장.\n",
    "- **특징**:\n",
    "  - 정적 크기 및 동적 크기 지원.\n",
    "  - GPU 및 TPU와 호환 가능.\n",
    "\n",
    "**3. `tf.RaggedTensor`**\n",
    "\n",
    "- **설명**: 불규칙한 길이를 가진 데이터를 표현하는 텐서.\n",
    "- **사용 예**: NLP(문장 길이가 다른 배치), 시퀀스 데이터.\n",
    "- **특징**:\n",
    "  - 행마다 길이가 다를 수 있음.\n",
    "  - 예: `[[1, 2], [3], [4, 5, 6]]` (모양: `[3, None]`).\n",
    "\n",
    "**4. `tf.string`**\n",
    "\n",
    "- **설명**: 문자열 데이터를 다루는 텐서.\n",
    "- **사용 예**: 텍스트 데이터 처리, 파일 경로 관리.\n",
    "- **특징**:\n",
    "  - 텐서 내부에 문자열 저장 가능.\n",
    "  - 문자열은 이진 데이터도 포함 가능.\n",
    "\n",
    "**5. `tf.sets`**\n",
    "\n",
    "- **설명**: 집합 연산을 위한 API 제공(예: 교집합, 합집합, 차집합).\n",
    "- **사용 예**: NLP에서 단어 집합 처리, 고유 데이터 계산.\n",
    "- **특징**:\n",
    "  - `SparseTensor` 기반으로 동작.\n",
    "  - `tf.sets.intersection`, `tf.sets.union` 등의 함수 제공.\n",
    "\n",
    "**6. `tf.queue`**\n",
    "\n",
    "- **설명**: 입력 데이터의 대기열(queue)을 처리. (TensorFlow 1.x에서 주로 사용됨.)\n",
    "- **사용 예**: 데이터 입력 파이프라인(큐에서 데이터 읽기/쓰기).\n",
    "- **특징**:\n",
    "  - FIFOQueue, RandomShuffleQueue 등의 구현 제공.\n",
    "  - **TensorFlow 2.x에서는 `tf.data` API**로 대체됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3 Customized Model and Training Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1 Customized Loss Function\n",
    "\n",
    "- **MSE** gives too much penalty for a large error, so it's very sensitive to noises or outliers\n",
    "- **MAE** gives too less penalty for a large error, so the training won't be good enough or too slow to train\n",
    "- **Huber** might be useful for this case\n",
    "- Many loss functions are already included in the keras, but pretend it doesn exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 2.5190 - mae: 2.0277 - val_loss: 0.4348 - val_mae: 0.6962\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.3911 - mae: 0.6600 - val_loss: 0.2791 - val_mae: 0.5451\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - loss: 0.2888 - mae: 0.5493 - val_loss: 0.2514 - val_mae: 0.4894\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.2400 - mae: 0.5004 - val_loss: 0.2085 - val_mae: 0.4625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a82e70e0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    \n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "# Save the model with customized loss\n",
    "model.save(\"./120/custom_loss_model.keras\")\n",
    "\n",
    "# Load the model with customized loss\n",
    "loaded_model = tf.keras.models.load_model(\"./120/custom_loss_model.keras\",\n",
    "                                   custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "loaded_model.compile(loss = HuberLoss(3.0), optimizer = 'adam')\n",
    "#  정상적으로 로드되고 모델을 정상적으로 사용할 수 있음을 보여줍니다.\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3 Customizing activation function, initializer, regulation, and limitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 3.0827 - mae: 1.2805 - val_loss: 0.7606 - val_mae: 0.6066\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.7635 - mae: 0.6057 - val_loss: 0.5594 - val_mae: 0.5060\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 0.5965 - mae: 0.5237 - val_loss: 0.4857 - val_mae: 0.4651\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.5292 - mae: 0.4865 - val_loss: 0.4530 - val_mae: 0.4453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factor': 0.01}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "\n",
    "# Define the customized functions\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):  # 반환 값은 tf.nn.relu(weights)입니다.\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "    \n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "    \n",
    "# train with the customized model\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=MyL1Regularizer(0.01))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"./120/my_model_with_many_custom_parts.keras\")\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_many_custom_parts.keras\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": MyL1Regularizer,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "loaded_model.layers[-1].kernel_regularizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.4 Customized metrics\n",
    "\n",
    "- loss and metric are similar but different\n",
    "- loss is used for training, differentiable and gradient is not for all domain\n",
    "- metric have no condition but it need to be easy to interpret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - huber_loss_5: 1.2148 - loss: 2.8134\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - huber_loss_5: 0.3255 - loss: 0.6772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b3322c90>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer='nadam', metrics = [HuberLoss(2.0)])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
