{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2 Use Tensorflow like Numpy\n",
    "\n",
    "tf.constant(): create immutable constant tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Constant Tensor\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "t.shape: (2, 3)\n",
      "t.dtype: <dtype: 'float32'>\n",
      "t[:, 1:] \n",
      " [[2. 3.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(\"1. Constant Tensor\")\n",
    "print(t) # Show variable\n",
    "print('')\n",
    "\n",
    "print(f't.shape: {t.shape}')\n",
    "print(f't.dtype: {t.dtype}')\n",
    "print(f't[:, 1:] \\n {t[:, 1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1 Operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "tf.square(t) \n",
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elementwise multiplication\n",
    "t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.squeeze\n",
    "\n",
    "- í¬ê¸°ê°€ 1ì¸ ì°¨ì›(Axis)ì„ ì œê±°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1]\n",
      "  [2]\n",
      "  [3]]], shape=(1, 3, 1), dtype=int32)\n",
      "shape before squeeze (1, 3, 1)\n",
      "shape after squeeze (3,)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[[1], [2], [3]]])  # Shape: (1, 3, 1)\n",
    "print(x)  # Output: [[[1], [2], [3]]]\n",
    "print(f\"shape before squeeze {x.shape}\")\n",
    "result = tf.squeeze(x)  # Shape: (3,)\n",
    "print(f\"shape after squeeze {result.shape}\")\n",
    "print(result)  # Output: [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.tile(input, multiples)\n",
    "\n",
    "- íŠ¹ì • ì°¨ì›(axis)ì„ ë”°ë¼ ë³µì œ(ë°˜ë³µ)í•©ë‹ˆë‹¤\n",
    "\n",
    "Parameters\n",
    "\n",
    "- input: ë³µì œí•  ëŒ€ìƒ í…ì„œ.\n",
    "- multiples: ê° ì¶•(axis)ì—ì„œ ë°˜ë³µí•  íšŸìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]\n",
      " [1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]], shape=(4, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2],\n",
    "                 [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "# Repeat 2 times along axis 0, 3 times along axis 1\n",
    "result = tf.tile(x, multiples=[2, 3])  # Shape: (4, 6)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Type Casting\n",
    "\n",
    "- tensorflow does not automatically cast any data types\n",
    "- any operation between other data type will raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.constant(40., dtype = tf.float64)\n",
    "t2 = tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.4 Variable\n",
    "\n",
    "- tf.constant is immutable\n",
    "- for weight update, need variable to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.,], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign\n",
    "\n",
    "- tf variable valus cannot be normally changed\n",
    "- need assign function to change variable values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1.00e+02, 4.20e+01, 1.00e-01],\n",
       "       [5.12e+02, 6.40e+02, 2.00e+02]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v) #[2., 4., 6.]\n",
    "v[0, 1].assign(42) # (0,1) element become 42\n",
    "v[:, 2].assign([0.1, 1.0]) # all rows at 2nd column become 0.1, 1.0\n",
    "\n",
    "# Assign new values for speicif elements\n",
    "v.scatter_nd_update(\n",
    "    indices = [[0,0], [1,2]], updates = [100., 200.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "**1. `tf.SparseTensor`**\n",
    "\n",
    "- **ì„¤ëª…**: í¬ì†Œ í…ì„œë¥¼ í‘œí˜„. ëŒ€ë¶€ë¶„ì˜ ê°’ì´ 0ì¸ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥.\n",
    "- **ì‚¬ìš© ì˜ˆ**: í¬ì†Œ í–‰ë ¬(ì˜ˆ: ì›-í•« ì¸ì½”ë”©, NLPì—ì„œ ë‹¨ì–´ ë²¡í„°).\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì¢Œí‘œ(`indices`), ê°’(`values`), ì „ì²´ ëª¨ì–‘(`dense_shape`)ìœ¼ë¡œ êµ¬ì„±.\n",
    "  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì„.\n",
    "\n",
    "**2. `tf.TensorArray`**\n",
    "\n",
    "- **ì„¤ëª…**: ê°€ë³€ ê¸¸ì´ì˜ í…ì„œ ë°°ì—´. ë°˜ë³µë¬¸ì´ë‚˜ ê·¸ë˜í”„ ëª¨ë“œì—ì„œ ë™ì ìœ¼ë¡œ í…ì„œë¥¼ ì €ì¥/ì¶”ì¶œ.\n",
    "- **ì‚¬ìš© ì˜ˆ**: RNN, ë™ì  ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ ì €ì¥.\n",
    "- **íŠ¹ì§•**:\n",
    "  - ì •ì  í¬ê¸° ë° ë™ì  í¬ê¸° ì§€ì›.\n",
    "  - GPU ë° TPUì™€ í˜¸í™˜ ê°€ëŠ¥.\n",
    "\n",
    "**3. `tf.RaggedTensor`**\n",
    "\n",
    "- **ì„¤ëª…**: ë¶ˆê·œì¹™í•œ ê¸¸ì´ë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” í…ì„œ.\n",
    "- **ì‚¬ìš© ì˜ˆ**: NLP(ë¬¸ì¥ ê¸¸ì´ê°€ ë‹¤ë¥¸ ë°°ì¹˜), ì‹œí€€ìŠ¤ ë°ì´í„°.\n",
    "- **íŠ¹ì§•**:\n",
    "  - í–‰ë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ.\n",
    "  - ì˜ˆ: `[[1, 2], [3], [4, 5, 6]]` (ëª¨ì–‘: `[3, None]`).\n",
    "\n",
    "**4. `tf.string`**\n",
    "\n",
    "- **ì„¤ëª…**: ë¬¸ìì—´ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” í…ì„œ.\n",
    "- **ì‚¬ìš© ì˜ˆ**: í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬, íŒŒì¼ ê²½ë¡œ ê´€ë¦¬.\n",
    "- **íŠ¹ì§•**:\n",
    "  - í…ì„œ ë‚´ë¶€ì— ë¬¸ìì—´ ì €ì¥ ê°€ëŠ¥.\n",
    "  - ë¬¸ìì—´ì€ ì´ì§„ ë°ì´í„°ë„ í¬í•¨ ê°€ëŠ¥.\n",
    "\n",
    "**5. `tf.sets`**\n",
    "\n",
    "- **ì„¤ëª…**: ì§‘í•© ì—°ì‚°ì„ ìœ„í•œ API ì œê³µ(ì˜ˆ: êµì§‘í•©, í•©ì§‘í•©, ì°¨ì§‘í•©).\n",
    "- **ì‚¬ìš© ì˜ˆ**: NLPì—ì„œ ë‹¨ì–´ ì§‘í•© ì²˜ë¦¬, ê³ ìœ  ë°ì´í„° ê³„ì‚°.\n",
    "- **íŠ¹ì§•**:\n",
    "  - `SparseTensor` ê¸°ë°˜ìœ¼ë¡œ ë™ì‘.\n",
    "  - `tf.sets.intersection`, `tf.sets.union` ë“±ì˜ í•¨ìˆ˜ ì œê³µ.\n",
    "\n",
    "**6. `tf.queue`**\n",
    "\n",
    "- **ì„¤ëª…**: ì…ë ¥ ë°ì´í„°ì˜ ëŒ€ê¸°ì—´(queue)ì„ ì²˜ë¦¬. (TensorFlow 1.xì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë¨.)\n",
    "- **ì‚¬ìš© ì˜ˆ**: ë°ì´í„° ì…ë ¥ íŒŒì´í”„ë¼ì¸(íì—ì„œ ë°ì´í„° ì½ê¸°/ì“°ê¸°).\n",
    "- **íŠ¹ì§•**:\n",
    "  - FIFOQueue, RandomShuffleQueue ë“±ì˜ êµ¬í˜„ ì œê³µ.\n",
    "  - **TensorFlow 2.xì—ì„œëŠ” `tf.data` API**ë¡œ ëŒ€ì²´ë¨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3 Customized Model and Training Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1 Customized Loss Function\n",
    "\n",
    "- **MSE** gives too much penalty for a large error, so it's very sensitive to noises or outliers\n",
    "- **MAE** gives too less penalty for a large error, so the training won't be good enough or too slow to train\n",
    "- **Huber** might be useful for this case\n",
    "- Many loss functions are already included in the keras, but pretend it doesn exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 2.5190 - mae: 2.0277 - val_loss: 0.4348 - val_mae: 0.6962\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.3911 - mae: 0.6600 - val_loss: 0.2791 - val_mae: 0.5451\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - loss: 0.2888 - mae: 0.5493 - val_loss: 0.2514 - val_mae: 0.4894\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.2400 - mae: 0.5004 - val_loss: 0.2085 - val_mae: 0.4625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a82e70e0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    \n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "# Save the model with customized loss\n",
    "model.save(\"./120/custom_loss_model.keras\")\n",
    "\n",
    "# Load the model with customized loss\n",
    "loaded_model = tf.keras.models.load_model(\"./120/custom_loss_model.keras\",\n",
    "                                   custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "loaded_model.compile(loss = HuberLoss(3.0), optimizer = 'adam')\n",
    "#  ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ê³  ëª¨ë¸ì„ ì •ìƒì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3 Customizing activation function, initializer, regulation, and limitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 3.0827 - mae: 1.2805 - val_loss: 0.7606 - val_mae: 0.6066\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.7635 - mae: 0.6057 - val_loss: 0.5594 - val_mae: 0.5060\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 0.5965 - mae: 0.5237 - val_loss: 0.4857 - val_mae: 0.4651\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.5292 - mae: 0.4865 - val_loss: 0.4530 - val_mae: 0.4453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factor': 0.01}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "\n",
    "# Define the customized functions\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):  # ë°˜í™˜ ê°’ì€ tf.nn.relu(weights)ì…ë‹ˆë‹¤.\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "    \n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "    \n",
    "# train with the customized model\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=MyL1Regularizer(0.01))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"./120/my_model_with_many_custom_parts.keras\")\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_many_custom_parts.keras\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": MyL1Regularizer,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "loaded_model.layers[-1].kernel_regularizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.4 Customized metrics\n",
    "\n",
    "- loss and metric are similar but different\n",
    "- loss is used for training, differentiable and gradient is not for all domain\n",
    "- metric have no condition but it need to be easy to interpret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - huber_loss_5: 1.2148 - loss: 2.8134\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - huber_loss_5: 0.3255 - loss: 0.6772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b3322c90>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer='nadam', metrics = [HuberLoss(2.0)])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Metric\n",
    "\n",
    "- update the metric as per batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunjaecho/Git/practical_ml_dl/env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step - custom_huber_metric: 0.5967 - loss: 0.2909\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - custom_huber_metric: 0.3324 - loss: 0.1654\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step - custom_huber_metric: 0.2548 - loss: 0.2548\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - custom_huber_metric: 0.2147 - loss: 0.2147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x291b20230>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# ğŸ”¹ ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)  # ë°ì´í„°ì…‹ì„ í•™ìŠµ/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í• \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)  # í•™ìŠµ ë°ì´í„°ì—ì„œ ê²€ì¦ ë°ì´í„° ë¶„í• \n",
    "\n",
    "# ğŸ”¹ ë°ì´í„° ì •ê·œí™” (í‘œì¤€í™”)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ğŸ”¹ ì…ë ¥ ë°ì´í„° ì°¨ì› ì„¤ì •\n",
    "input_shape = X_train.shape[1:]  # (íŠ¹ì„± ê°œìˆ˜,)\n",
    "\n",
    "#  Huber ì†ì‹¤ í•¨ìˆ˜ (ì†ì‹¤ì´ í¬ë©´ MAE, ì‘ìœ¼ë©´ MSE ì ìš©)\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Huber ì†ì‹¤ í•¨ìˆ˜ ì´ˆê¸°í™”\n",
    "        Args:\n",
    "            threshold (float): MSEì™€ MAEë¥¼ êµ¬ë¶„í•˜ëŠ” ì„ê³„ê°’\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)  # ë¶€ëª¨ í´ë˜ìŠ¤(tf.keras.losses.Loss) ì´ˆê¸°í™”\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        ì†ì‹¤ ê°’ ê³„ì‚°\n",
    "        Args:\n",
    "            y_true: ì‹¤ì œ ê°’ (ì •ë‹µ)\n",
    "            y_pred: ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’\n",
    "        Returns:\n",
    "            Tensor: Huber ì†ì‹¤ ê°’\n",
    "        \"\"\"\n",
    "        error = y_true - y_pred  # ì˜ˆì¸¡ ì˜¤ì°¨ ê³„ì‚°\n",
    "        is_small_error = tf.abs(error) < self.threshold  # ì˜¤ì°¨ í¬ê¸° ë¹„êµ\n",
    "\n",
    "        squared_loss = tf.square(error) / 2  # ì‘ì€ ì˜¤ì°¨ì¼ ë•Œ MSE ë°©ì‹\n",
    "        linear_loss = self.threshold * (tf.abs(error) - self.threshold / 2)  # í° ì˜¤ì°¨ì¼ ë•Œ MAE ë°©ì‹\n",
    "\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)  # ë‘ ì†ì‹¤ ì¤‘ í•´ë‹¹ë˜ëŠ” ê°’ ë°˜í™˜\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ì €ì¥ ì‹œ ì„¤ì • ì •ë³´ ë°˜í™˜\n",
    "        Returns:\n",
    "            dict: ì„¤ì • ê°’ (threshold í¬í•¨)\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "#  Huber ì†ì‹¤ì„ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ (HuberLossì™€ ë™ì¼í•œ ìˆ˜ì‹ ì‚¬ìš©)\n",
    "def create_huber(threshold=1.0):\n",
    "    \"\"\"\n",
    "    Huber ì†ì‹¤ ê³„ì‚°ì„ ìœ„í•œ í•¨ìˆ˜ (Metric ìš©)\n",
    "    Args:\n",
    "        threshold (float): MSEì™€ MAEë¥¼ êµ¬ë¶„í•˜ëŠ” ì„ê³„ê°’\n",
    "    Returns:\n",
    "        function: ì†ì‹¤ ê³„ì‚° í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * (tf.abs(error) - threshold / 2)\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    return huber_fn\n",
    "\n",
    "#  Huber ì†ì‹¤ì„ í‰ê·  ê³„ì‚°í•˜ì—¬ í‰ê°€ ì§€í‘œ(Metric)ë¡œ í™œìš©\n",
    "class HuberMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='custom_huber_metric', dtype=None):\n",
    "        \"\"\"\n",
    "        Huber ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í‰ê·  ë©”íŠ¸ë¦­\n",
    "        Args:\n",
    "            threshold (float): MSEì™€ MAEë¥¼ êµ¬ë¶„í•˜ëŠ” ì„ê³„ê°’\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)  # Huber ì†ì‹¤ í•¨ìˆ˜ ìƒì„±\n",
    "        super().__init__(name=name, dtype=dtype)  # ë¶€ëª¨ í´ë˜ìŠ¤(tf.keras.metrics.Mean) ì´ˆê¸°í™”\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        ë°°ì¹˜(batch) ë‹¨ìœ„ë¡œ ë©”íŠ¸ë¦­ ê°’ì„ ì—…ë°ì´íŠ¸\n",
    "        Args:\n",
    "            y_true: ì‹¤ì œ ê°’\n",
    "            y_pred: ì˜ˆì¸¡ ê°’\n",
    "            sample_weight (Tensor, optional): ìƒ˜í”Œ ê°€ì¤‘ì¹˜ (ê° ë°ì´í„° í¬ì¸íŠ¸ë³„ ì¤‘ìš”ë„ ì¡°ì •)\n",
    "        \"\"\"\n",
    "        metric = self.huber_fn(y_true, y_pred)  # Huber ì†ì‹¤ ê³„ì‚°\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)  # í‰ê·  ê°’ ì—…ë°ì´íŠ¸\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ì €ì¥ ì‹œ ì„¤ì • ì •ë³´ ë°˜í™˜\n",
    "        Returns:\n",
    "            dict: ì„¤ì • ê°’ (threshold í¬í•¨)\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# ğŸ”¹ ëœë¤ ì‹œë“œ ì„¤ì • (ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#  ëª¨ë¸ ìƒì„± (Dense ë ˆì´ì–´ 2ê°œ ì‚¬ìš©)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),  # ì²« ë²ˆì§¸ Dense ë ˆì´ì–´ (30ê°œ ë‰´ëŸ°, He ì´ˆê¸°í™”)\n",
    "    tf.keras.layers.Dense(1),  # ì¶œë ¥ì¸µ (1ê°œì˜ ë‰´ëŸ°, íšŒê·€ ë¬¸ì œ)\n",
    "])\n",
    "\n",
    "#  ëª¨ë¸ ì»´íŒŒì¼ (ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ì •)\n",
    "model.compile(loss=HuberLoss(2.0),  # Huber ì†ì‹¤ ì ìš©\n",
    "              optimizer=\"nadam\",  # Nadam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "              metrics=[HuberMetric(2.0)])  # Huber ë©”íŠ¸ë¦­ ì ìš©\n",
    "\n",
    "#  ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ìƒì„± (ëœë¤ ê°€ì¤‘ì¹˜ ë¶€ì—¬)\n",
    "sample_weight = np.random.rand(len(y_train))\n",
    "\n",
    "#  ëª¨ë¸ í•™ìŠµ\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2,\n",
    "                    sample_weight=sample_weight)  # ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì ìš©í•˜ì—¬ í•™ìŠµ\n",
    "\n",
    "#  í•™ìŠµ ì†ì‹¤ ë° ë©”íŠ¸ë¦­ í™•ì¸\n",
    "print(\n",
    "    history.history[\"loss\"][0],  # ì²« ë²ˆì§¸ ì—í¬í¬ ì†ì‹¤\n",
    "    history.history[\"custom_huber_metric\"][0] * sample_weight.mean()  # ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•œ í‰ê·  ë©”íŠ¸ë¦­\n",
    ")\n",
    "\n",
    "#  ëª¨ë¸ ì €ì¥\n",
    "model.save(\"./120/my_model_with_a_custom_metric.keras\")\n",
    "\n",
    "#  ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ (ì»¤ìŠ¤í…€ ê°ì²´ í¬í•¨)\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_a_custom_metric.keras\",\n",
    "    custom_objects={\n",
    "        \"HuberLoss\": HuberLoss,\n",
    "        \"HuberMetric\": HuberMetric\n",
    "    }\n",
    ")\n",
    "\n",
    "#  ë¡œë“œí•œ ëª¨ë¸ ì¶”ê°€ í•™ìŠµ\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_huber_metric': [1.2572898864746094, 0.3699382245540619],\n",
       " 'loss': [0.6316739916801453, 0.18845683336257935]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CompileMetrics name=compile_metrics>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HuberLoss at 0x287fe6090>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lossx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.5 Custom Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 4.2519 - val_loss: 0.8888\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - loss: 0.7923 - val_loss: 0.6242\n",
      "\u001b[1m162/162\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 0.6235\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.6419\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221us/step - loss: 0.5540\n",
      "\u001b[1m162/162\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.26484483],\n",
       "       [1.2860062 ],\n",
       "       [3.5598931 ],\n",
       "       ...,\n",
       "       [1.2615829 ],\n",
       "       [2.261335  ],\n",
       "       [3.5348647 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyDense(tf.keras.layers.Layer):  \n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©ì ì •ì˜ Dense ë ˆì´ì–´ ì´ˆê¸°í™” í•¨ìˆ˜\n",
    "        \n",
    "        Args:\n",
    "            units (int): ë‰´ëŸ° ê°œìˆ˜ (ì¶œë ¥ ì°¨ì›)\n",
    "            activation (str ë˜ëŠ” í•¨ìˆ˜, optional): ì‚¬ìš©í•  í™œì„±í™” í•¨ìˆ˜ (ì˜ˆ: \"relu\", \"sigmoid\")\n",
    "            **kwargs: ì¶”ê°€ì ì¸ Keras Layer ì¸ì (ì˜ˆ: name, dtype)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)  # ğŸ”¹ ë¶€ëª¨ í´ë˜ìŠ¤(tf.keras.layers.Layer)ì˜ ì´ˆê¸°í™” ë©”ì„œë“œ í˜¸ì¶œ\n",
    "        self.units = units  # ğŸ”¹ ì¶œë ¥ ë‰´ëŸ° ê°œìˆ˜ ì €ì¥\n",
    "        self.activation = tf.keras.activations.get(activation)  # ğŸ”¹ í™œì„±í™” í•¨ìˆ˜ ì§€ì • (Keras ì œê³µ í•¨ìˆ˜ ì‚¬ìš©)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        \"\"\"\n",
    "        ì…ë ¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ (kernel)ì™€ í¸í–¥ (bias) ì •ì˜\n",
    "\n",
    "        Args:\n",
    "            batch_input_shape (tuple): ì…ë ¥ ë°ì´í„°ì˜ shape (ë°°ì¹˜ í¬ê¸° í¬í•¨)\n",
    "        \"\"\"\n",
    "        # ğŸ”¹ ê°€ì¤‘ì¹˜ í–‰ë ¬(kernel) ìƒì„± â†’ (ì…ë ¥ ì°¨ì›, ì¶œë ¥ ì°¨ì›)\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",  # ë³€ìˆ˜ëª… ì§€ì •\n",
    "            shape=[batch_input_shape[-1], self.units],  # (ì…ë ¥ íŠ¹ì„± ê°œìˆ˜, ì¶œë ¥ ë‰´ëŸ° ê°œìˆ˜)\n",
    "            initializer=\"he_normal\"  # He ì´ˆê¸°í™” ì‚¬ìš© (ReLU í™œì„±í™” í•¨ìˆ˜ì— ì í•©)\n",
    "        )\n",
    "\n",
    "        # ğŸ”¹ í¸í–¥ ë²¡í„°(bias) ìƒì„± â†’ (ì¶œë ¥ ì°¨ì›,)\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=[self.units],  # í¸í–¥ì€ ì¶œë ¥ ë‰´ëŸ° ê°œìˆ˜ë§Œí¼ ì¡´ì¬\n",
    "            initializer=\"zeros\"  # ê¸°ë³¸ì ìœ¼ë¡œ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        )\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        ìˆœì „íŒŒ (Forward Propagation) ì—°ì‚° ìˆ˜í–‰\n",
    "        Args:\n",
    "            X (Tensor): ì…ë ¥ ë°ì´í„° (batch_size, input_dim)\n",
    "        Returns:\n",
    "            Tensor: í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•œ ì¶œë ¥ ê²°ê³¼ (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # ğŸ”¹ Dense ë ˆì´ì–´ ìˆ˜ì‹ ì ìš©: Y = activation(X @ W + b)\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì‹œ í•„ìš”í•œ ì„¤ì • ë°˜í™˜\n",
    "        Returns:\n",
    "            dict: ê°ì²´ì˜ ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()  # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        return {\n",
    "            **base_config,  # ê¸°ì¡´ ì„¤ì • ìœ ì§€\n",
    "            \"units\": self.units,  # ë‰´ëŸ° ê°œìˆ˜ ì¶”ê°€ ì €ì¥\n",
    "            \"activation\": tf.keras.activations.serialize(self.activation)  # í™œì„±í™” í•¨ìˆ˜ ì§ë ¬í™”\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "# ì¶”ê°€ ì½”ë“œ - ì‚¬ìš©ì ì •ì˜ ì¸µì„ ë³´í†µì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    MyDense(30, activation=\"relu\"),\n",
    "    MyDense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"./120/my_model_with_a_custom_layer.keras\")\n",
    "\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_a_custom_layer.keras\",\n",
    "    custom_objects={\n",
    "        \"MyDense\":MyDense\n",
    "    }\n",
    ")\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2)\n",
    "loaded_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "\n",
    "**`keras.Model`ì„ ìƒì†í•œ ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì„± ìš”ì†Œ**\n",
    "| êµ¬ì„± ìš”ì†Œ | ì„¤ëª… | ì£¼ìš” ì—­í•  |\n",
    "|-----------|--------------------------------|------------------|\n",
    "| `__init__()` | ëª¨ë¸ì˜ ì¸µ(layer)ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ | ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ìƒì„± |\n",
    "| `call(inputs)` | ìˆœì „íŒŒ(forward pass) ì •ì˜ | ë°ì´í„° ì…ë ¥ â†’ ì¶œë ¥ ë³€í™˜ |\n",
    "| `build(input_shape)` | ì…ë ¥ ë°ì´í„° í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” | ê°€ì¤‘ì¹˜ ìƒì„± ë° ë°°ì¹˜ í¬ê¸° ê³ ë ¤ |\n",
    "| `compile()` | ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, í‰ê°€ ì§€í‘œ ì„¤ì • | í•™ìŠµì„ ìœ„í•œ ì„¤ì • |\n",
    "| `fit()` | ë°ì´í„° í•™ìŠµ | ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰ |\n",
    "| `evaluate()` | í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ | ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ |\n",
    "| `predict()` | ì…ë ¥ ë°ì´í„° ì˜ˆì¸¡ | ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ |\n",
    "| `get_config()` | ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì‹œ ì„¤ì • ìœ ì§€ | ëª¨ë¸ ì§ë ¬í™” ì§€ì› |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©ì ì •ì˜ CNN ëª¨ë¸ (Keras Model ìƒì†)\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): ì¶œë ¥ í´ë˜ìŠ¤ ê°œìˆ˜\n",
    "            dropout_rate (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "            **kwargs: ì¶”ê°€ì ì¸ Keras Model ì„¤ì •\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # ğŸ”¹ CNN ê¸°ë°˜ ì‹ ê²½ë§ ì •ì˜\n",
    "        self.conv1 = layers.Conv2D(32, (3, 3), activation='relu', name=\"conv1\")\n",
    "        self.pool1 = layers.MaxPooling2D((2, 2), name=\"pool1\")\n",
    "        self.conv2 = layers.Conv2D(64, (3, 3), activation='relu', name=\"conv2\")\n",
    "        self.pool2 = layers.MaxPooling2D((2, 2), name=\"pool2\")\n",
    "        self.flatten = layers.Flatten(name=\"flatten\")\n",
    "        self.fc1 = layers.Dense(128, activation='relu', name=\"fc1\")\n",
    "        self.dropout = layers.Dropout(self.dropout_rate, name=\"dropout\")\n",
    "        self.fc2 = layers.Dense(num_classes, activation='softmax', name=\"fc2\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        ìˆœì „íŒŒ(Forward Propagation) ì •ì˜\n",
    "        \"\"\"\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì‹œ í•„ìš”í•œ ì„¤ì • ì •ë³´ ë°˜í™˜\n",
    "        Returns:\n",
    "            dict: ëª¨ë¸ì˜ ì„¤ì • ê°’ ì €ì¥ (num_classes, dropout_rate ë“±)\n",
    "        \"\"\"\n",
    "        config = super().get_config()  # ë¶€ëª¨ í´ë˜ìŠ¤(Keras Model)ì˜ ì„¤ì • ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,  # í´ë˜ìŠ¤ ê°œìˆ˜ ì €ì¥\n",
    "            \"dropout_rate\": self.dropout_rate  # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ì €ì¥\n",
    "        })\n",
    "        return config  # ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
