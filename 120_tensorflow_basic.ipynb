{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.2 Use Tensorflow like Numpy\n",
    "\n",
    "tf.constant(): create immutable constant tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Constant Tensor\n",
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "\n",
      "t.shape: (2, 3)\n",
      "t.dtype: <dtype: 'float32'>\n",
      "t[:, 1:] \n",
      " [[2. 3.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "\n",
    "print(\"1. Constant Tensor\")\n",
    "print(t) # Show variable\n",
    "print('')\n",
    "\n",
    "print(f't.shape: {t.shape}')\n",
    "print(f't.dtype: {t.dtype}')\n",
    "print(f't[:, 1:] \\n {t[:, 1:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1 Operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix Multiplication\n",
    "tf.square(t) \n",
    "t @ tf.transpose(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[ 1.,  4.,  9.],\n",
       "       [16., 25., 36.]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elementwise multiplication\n",
    "t * t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.squeeze\n",
    "\n",
    "- 크기가 1인 차원(Axis)을 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[1]\n",
      "  [2]\n",
      "  [3]]], shape=(1, 3, 1), dtype=int32)\n",
      "shape before squeeze (1, 3, 1)\n",
      "shape after squeeze (3,)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[[1], [2], [3]]])  # Shape: (1, 3, 1)\n",
    "print(x)  # Output: [[[1], [2], [3]]]\n",
    "print(f\"shape before squeeze {x.shape}\")\n",
    "result = tf.squeeze(x)  # Shape: (3,)\n",
    "print(f\"shape after squeeze {result.shape}\")\n",
    "print(result)  # Output: [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.tile(input, multiples)\n",
    "\n",
    "- 특정 차원(axis)을 따라 복제(반복)합니다\n",
    "\n",
    "Parameters\n",
    "\n",
    "- input: 복제할 대상 텐서.\n",
    "- multiples: 각 축(axis)에서 반복할 횟수를 나타내는 정수 리스트.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]\n",
      " [1 2 1 2 1 2]\n",
      " [3 4 3 4 3 4]], shape=(4, 6), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1, 2],\n",
    "                 [3, 4]])  # Shape: (2, 2)\n",
    "\n",
    "# Repeat 2 times along axis 0, 3 times along axis 1\n",
    "result = tf.tile(x, multiples=[2, 3])  # Shape: (4, 6)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Type Casting\n",
    "\n",
    "- tensorflow does not automatically cast any data types\n",
    "- any operation between other data type will raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.constant(40., dtype = tf.float64)\n",
    "t2 = tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.4 Variable\n",
    "\n",
    "- tf.constant is immutable\n",
    "- for weight update, need variable to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.,], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assign\n",
    "\n",
    "- tf variable valus cannot be normally changed\n",
    "- need assign function to change variable values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1.00e+02, 4.20e+01, 1.00e-01],\n",
       "       [5.12e+02, 6.40e+02, 2.00e+02]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.assign(2 * v) #[2., 4., 6.]\n",
    "v[0, 1].assign(42) # (0,1) element become 42\n",
    "v[:, 2].assign([0.1, 1.0]) # all rows at 2nd column become 0.1, 1.0\n",
    "\n",
    "# Assign new values for speicif elements\n",
    "v.scatter_nd_update(\n",
    "    indices = [[0,0], [1,2]], updates = [100., 200.]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "**1. `tf.SparseTensor`**\n",
    "\n",
    "- **설명**: 희소 텐서를 표현. 대부분의 값이 0인 데이터를 효율적으로 저장.\n",
    "- **사용 예**: 희소 행렬(예: 원-핫 인코딩, NLP에서 단어 벡터).\n",
    "- **특징**:\n",
    "  - 좌표(`indices`), 값(`values`), 전체 모양(`dense_shape`)으로 구성.\n",
    "  - 메모리 사용량을 줄임.\n",
    "\n",
    "**2. `tf.TensorArray`**\n",
    "\n",
    "- **설명**: 가변 길이의 텐서 배열. 반복문이나 그래프 모드에서 동적으로 텐서를 저장/추출.\n",
    "- **사용 예**: RNN, 동적 계산 그래프에서 중간 계산 결과 저장.\n",
    "- **특징**:\n",
    "  - 정적 크기 및 동적 크기 지원.\n",
    "  - GPU 및 TPU와 호환 가능.\n",
    "\n",
    "**3. `tf.RaggedTensor`**\n",
    "\n",
    "- **설명**: 불규칙한 길이를 가진 데이터를 표현하는 텐서.\n",
    "- **사용 예**: NLP(문장 길이가 다른 배치), 시퀀스 데이터.\n",
    "- **특징**:\n",
    "  - 행마다 길이가 다를 수 있음.\n",
    "  - 예: `[[1, 2], [3], [4, 5, 6]]` (모양: `[3, None]`).\n",
    "\n",
    "**4. `tf.string`**\n",
    "\n",
    "- **설명**: 문자열 데이터를 다루는 텐서.\n",
    "- **사용 예**: 텍스트 데이터 처리, 파일 경로 관리.\n",
    "- **특징**:\n",
    "  - 텐서 내부에 문자열 저장 가능.\n",
    "  - 문자열은 이진 데이터도 포함 가능.\n",
    "\n",
    "**5. `tf.sets`**\n",
    "\n",
    "- **설명**: 집합 연산을 위한 API 제공(예: 교집합, 합집합, 차집합).\n",
    "- **사용 예**: NLP에서 단어 집합 처리, 고유 데이터 계산.\n",
    "- **특징**:\n",
    "  - `SparseTensor` 기반으로 동작.\n",
    "  - `tf.sets.intersection`, `tf.sets.union` 등의 함수 제공.\n",
    "\n",
    "**6. `tf.queue`**\n",
    "\n",
    "- **설명**: 입력 데이터의 대기열(queue)을 처리. (TensorFlow 1.x에서 주로 사용됨.)\n",
    "- **사용 예**: 데이터 입력 파이프라인(큐에서 데이터 읽기/쓰기).\n",
    "- **특징**:\n",
    "  - FIFOQueue, RandomShuffleQueue 등의 구현 제공.\n",
    "  - **TensorFlow 2.x에서는 `tf.data` API**로 대체됨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.3 Customized Model and Training Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.1 Customized Loss Function\n",
    "\n",
    "- **MSE** gives too much penalty for a large error, so it's very sensitive to noises or outliers\n",
    "- **MAE** gives too less penalty for a large error, so the training won't be good enough or too slow to train\n",
    "- **Huber** might be useful for this case\n",
    "- Many loss functions are already included in the keras, but pretend it doesn exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462us/step - loss: 2.5190 - mae: 2.0277 - val_loss: 0.4348 - val_mae: 0.6962\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 320us/step - loss: 0.3911 - mae: 0.6600 - val_loss: 0.2791 - val_mae: 0.5451\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - loss: 0.2888 - mae: 0.5493 - val_loss: 0.2514 - val_mae: 0.4894\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - loss: 0.2400 - mae: 0.5004 - val_loss: 0.2085 - val_mae: 0.4625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a82e70e0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    \n",
    "\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "# Save the model with customized loss\n",
    "model.save(\"./120/custom_loss_model.keras\")\n",
    "\n",
    "# Load the model with customized loss\n",
    "loaded_model = tf.keras.models.load_model(\"./120/custom_loss_model.keras\",\n",
    "                                   custom_objects={\"HuberLoss\": HuberLoss})\n",
    "\n",
    "loaded_model.compile(loss = HuberLoss(3.0), optimizer = 'adam')\n",
    "#  정상적으로 로드되고 모델을 정상적으로 사용할 수 있음을 보여줍니다.\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3 Customizing activation function, initializer, regulation, and limitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467us/step - loss: 3.0827 - mae: 1.2805 - val_loss: 0.7606 - val_mae: 0.6066\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - loss: 0.7635 - mae: 0.6057 - val_loss: 0.5594 - val_mae: 0.5060\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - loss: 0.5965 - mae: 0.5237 - val_loss: 0.4857 - val_mae: 0.4651\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330us/step - loss: 0.5292 - mae: 0.4865 - val_loss: 0.4530 - val_mae: 0.4453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'factor': 0.01}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "\n",
    "\n",
    "# Define the customized functions\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(1.0 + tf.exp(z))\n",
    "\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "def my_positive_weights(weights):  # 반환 값은 tf.nn.relu(weights)입니다.\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "@keras.utils.register_keras_serializable()\n",
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "    \n",
    "\n",
    "input_shape = X_train_scaled.shape[1:]\n",
    "    \n",
    "# train with the customized model\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(1, activation=my_softplus,\n",
    "                          kernel_initializer=my_glorot_initializer,\n",
    "                          kernel_regularizer=MyL1Regularizer(0.01))\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "model.save(\"./120/my_model_with_many_custom_parts.keras\")\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_many_custom_parts.keras\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": MyL1Regularizer,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    }\n",
    ")\n",
    "\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "\n",
    "\n",
    "loaded_model.layers[-1].kernel_regularizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.4 Customized metrics\n",
    "\n",
    "- loss and metric are similar but different\n",
    "- loss is used for training, differentiable and gradient is not for all domain\n",
    "- metric have no condition but it need to be easy to interpret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - huber_loss_5: 1.2148 - loss: 2.8134\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239us/step - huber_loss_5: 0.3255 - loss: 0.6772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b3322c90>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer='nadam', metrics = [HuberLoss(2.0)])\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Metric\n",
    "\n",
    "- update the metric as per batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunjaecho/Git/practical_ml_dl/env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 283us/step - custom_huber_metric: 0.5967 - loss: 0.2909\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247us/step - custom_huber_metric: 0.3324 - loss: 0.1654\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264us/step - custom_huber_metric: 0.2548 - loss: 0.2548\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249us/step - custom_huber_metric: 0.2147 - loss: 0.2147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x291b20230>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 🔹 캘리포니아 주택 가격 데이터 로드 및 전처리\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)  # 데이터셋을 학습/테스트로 분할\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)  # 학습 데이터에서 검증 데이터 분할\n",
    "\n",
    "# 🔹 데이터 정규화 (표준화)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 🔹 입력 데이터 차원 설정\n",
    "input_shape = X_train.shape[1:]  # (특성 개수,)\n",
    "\n",
    "#  Huber 손실 함수 (손실이 크면 MAE, 작으면 MSE 적용)\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        \"\"\"\n",
    "        Huber 손실 함수 초기화\n",
    "        Args:\n",
    "            threshold (float): MSE와 MAE를 구분하는 임계값\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)  # 부모 클래스(tf.keras.losses.Loss) 초기화\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        손실 값 계산\n",
    "        Args:\n",
    "            y_true: 실제 값 (정답)\n",
    "            y_pred: 모델의 예측 값\n",
    "        Returns:\n",
    "            Tensor: Huber 손실 값\n",
    "        \"\"\"\n",
    "        error = y_true - y_pred  # 예측 오차 계산\n",
    "        is_small_error = tf.abs(error) < self.threshold  # 오차 크기 비교\n",
    "\n",
    "        squared_loss = tf.square(error) / 2  # 작은 오차일 때 MSE 방식\n",
    "        linear_loss = self.threshold * (tf.abs(error) - self.threshold / 2)  # 큰 오차일 때 MAE 방식\n",
    "\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)  # 두 손실 중 해당되는 값 반환\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        모델 저장 시 설정 정보 반환\n",
    "        Returns:\n",
    "            dict: 설정 값 (threshold 포함)\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "#  Huber 손실을 메트릭으로 사용하기 위한 함수 (HuberLoss와 동일한 수식 사용)\n",
    "def create_huber(threshold=1.0):\n",
    "    \"\"\"\n",
    "    Huber 손실 계산을 위한 함수 (Metric 용)\n",
    "    Args:\n",
    "        threshold (float): MSE와 MAE를 구분하는 임계값\n",
    "    Returns:\n",
    "        function: 손실 계산 함수\n",
    "    \"\"\"\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * (tf.abs(error) - threshold / 2)\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    \n",
    "    return huber_fn\n",
    "\n",
    "#  Huber 손실을 평균 계산하여 평가 지표(Metric)로 활용\n",
    "class HuberMetric(tf.keras.metrics.Mean):\n",
    "    def __init__(self, threshold=1.0, name='custom_huber_metric', dtype=None):\n",
    "        \"\"\"\n",
    "        Huber 손실을 기반으로 한 평균 메트릭\n",
    "        Args:\n",
    "            threshold (float): MSE와 MAE를 구분하는 임계값\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(threshold)  # Huber 손실 함수 생성\n",
    "        super().__init__(name=name, dtype=dtype)  # 부모 클래스(tf.keras.metrics.Mean) 초기화\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        배치(batch) 단위로 메트릭 값을 업데이트\n",
    "        Args:\n",
    "            y_true: 실제 값\n",
    "            y_pred: 예측 값\n",
    "            sample_weight (Tensor, optional): 샘플 가중치 (각 데이터 포인트별 중요도 조정)\n",
    "        \"\"\"\n",
    "        metric = self.huber_fn(y_true, y_pred)  # Huber 손실 계산\n",
    "        super(HuberMetric, self).update_state(metric, sample_weight)  # 평균 값 업데이트\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        모델 저장 시 설정 정보 반환\n",
    "        Returns:\n",
    "            dict: 설정 값 (threshold 포함)\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "\n",
    "# 🔹 랜덤 시드 설정 (재현 가능성 확보)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "#  모델 생성 (Dense 레이어 2개 사용)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                          input_shape=input_shape),  # 첫 번째 Dense 레이어 (30개 뉴런, He 초기화)\n",
    "    tf.keras.layers.Dense(1),  # 출력층 (1개의 뉴런, 회귀 문제)\n",
    "])\n",
    "\n",
    "#  모델 컴파일 (손실 함수, 옵티마이저, 평가 메트릭 설정)\n",
    "model.compile(loss=HuberLoss(2.0),  # Huber 손실 적용\n",
    "              optimizer=\"nadam\",  # Nadam 옵티마이저 사용\n",
    "              metrics=[HuberMetric(2.0)])  # Huber 메트릭 적용\n",
    "\n",
    "#  샘플 가중치 생성 (랜덤 가중치 부여)\n",
    "sample_weight = np.random.rand(len(y_train))\n",
    "\n",
    "#  모델 학습\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2,\n",
    "                    sample_weight=sample_weight)  # 샘플 가중치 적용하여 학습\n",
    "\n",
    "#  학습 손실 및 메트릭 확인\n",
    "print(\n",
    "    history.history[\"loss\"][0],  # 첫 번째 에포크 손실\n",
    "    history.history[\"custom_huber_metric\"][0] * sample_weight.mean()  # 가중치를 반영한 평균 메트릭\n",
    ")\n",
    "\n",
    "#  모델 저장\n",
    "model.save(\"./120/my_model_with_a_custom_metric.keras\")\n",
    "\n",
    "#  저장된 모델 로드 (커스텀 객체 포함)\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_a_custom_metric.keras\",\n",
    "    custom_objects={\n",
    "        \"HuberLoss\": HuberLoss,\n",
    "        \"HuberMetric\": HuberMetric\n",
    "    }\n",
    ")\n",
    "\n",
    "#  로드한 모델 추가 학습\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'custom_huber_metric': [1.2572898864746094, 0.3699382245540619],\n",
       " 'loss': [0.6316739916801453, 0.18845683336257935]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CompileMetrics name=compile_metrics>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.HuberLoss at 0x287fe6090>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lossx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.5 Custom Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463us/step - loss: 4.2519 - val_loss: 0.8888\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - loss: 0.7923 - val_loss: 0.6242\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step - loss: 0.6235\n",
      "Epoch 1/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 262us/step - loss: 0.6419\n",
      "Epoch 2/2\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221us/step - loss: 0.5540\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.26484483],\n",
       "       [1.2860062 ],\n",
       "       [3.5598931 ],\n",
       "       ...,\n",
       "       [1.2615829 ],\n",
       "       [2.261335  ],\n",
       "       [3.5348647 ]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyDense(tf.keras.layers.Layer):  \n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        \"\"\"\n",
    "        사용자 정의 Dense 레이어 초기화 함수\n",
    "        \n",
    "        Args:\n",
    "            units (int): 뉴런 개수 (출력 차원)\n",
    "            activation (str 또는 함수, optional): 사용할 활성화 함수 (예: \"relu\", \"sigmoid\")\n",
    "            **kwargs: 추가적인 Keras Layer 인자 (예: name, dtype)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)  # 🔹 부모 클래스(tf.keras.layers.Layer)의 초기화 메서드 호출\n",
    "        self.units = units  # 🔹 출력 뉴런 개수 저장\n",
    "        self.activation = tf.keras.activations.get(activation)  # 🔹 활성화 함수 지정 (Keras 제공 함수 사용)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        \"\"\"\n",
    "        입력 데이터를 기반으로 가중치 (kernel)와 편향 (bias) 정의\n",
    "\n",
    "        Args:\n",
    "            batch_input_shape (tuple): 입력 데이터의 shape (배치 크기 포함)\n",
    "        \"\"\"\n",
    "        # 🔹 가중치 행렬(kernel) 생성 → (입력 차원, 출력 차원)\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\",  # 변수명 지정\n",
    "            shape=[batch_input_shape[-1], self.units],  # (입력 특성 개수, 출력 뉴런 개수)\n",
    "            initializer=\"he_normal\"  # He 초기화 사용 (ReLU 활성화 함수에 적합)\n",
    "        )\n",
    "\n",
    "        # 🔹 편향 벡터(bias) 생성 → (출력 차원,)\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\",\n",
    "            shape=[self.units],  # 편향은 출력 뉴런 개수만큼 존재\n",
    "            initializer=\"zeros\"  # 기본적으로 0으로 초기화\n",
    "        )\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        순전파 (Forward Propagation) 연산 수행\n",
    "        Args:\n",
    "            X (Tensor): 입력 데이터 (batch_size, input_dim)\n",
    "        Returns:\n",
    "            Tensor: 활성화 함수를 적용한 출력 결과 (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # 🔹 Dense 레이어 수식 적용: Y = activation(X @ W + b)\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        모델 저장 및 로드 시 필요한 설정 반환\n",
    "        Returns:\n",
    "            dict: 객체의 설정 정보 딕셔너리\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()  # 부모 클래스의 설정 정보 가져오기\n",
    "        return {\n",
    "            **base_config,  # 기존 설정 유지\n",
    "            \"units\": self.units,  # 뉴런 개수 추가 저장\n",
    "            \"activation\": tf.keras.activations.serialize(self.activation)  # 활성화 함수 직렬화\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n",
    "# 추가 코드 - 사용자 정의 층을 보통처럼 사용할 수 있음을 보여줍니다.\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(input_shape),\n",
    "    MyDense(30, activation=\"relu\"),\n",
    "    MyDense(1)\n",
    "])\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)\n",
    "model.save(\"./120/my_model_with_a_custom_layer.keras\")\n",
    "\n",
    "\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    \"./120/my_model_with_a_custom_layer.keras\",\n",
    "    custom_objects={\n",
    "        \"MyDense\":MyDense\n",
    "    }\n",
    ")\n",
    "loaded_model.fit(X_train_scaled, y_train, epochs=2)\n",
    "loaded_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model\n",
    "\n",
    "**`keras.Model`을 상속한 커스텀 모델 구성 요소**\n",
    "| 구성 요소 | 설명 | 주요 역할 |\n",
    "|-----------|--------------------------------|------------------|\n",
    "| `__init__()` | 모델의 층(layer)과 하이퍼파라미터 정의 | 네트워크 구조 생성 |\n",
    "| `call(inputs)` | 순전파(forward pass) 정의 | 데이터 입력 → 출력 변환 |\n",
    "| `build(input_shape)` | 입력 데이터 크기를 기반으로 가중치 초기화 | 가중치 생성 및 배치 크기 고려 |\n",
    "| `compile()` | 손실 함수, 옵티마이저, 평가 지표 설정 | 학습을 위한 설정 |\n",
    "| `fit()` | 데이터 학습 | 모델 훈련 실행 |\n",
    "| `evaluate()` | 테스트 데이터 평가 | 모델 성능 평가 |\n",
    "| `predict()` | 입력 데이터 예측 | 새로운 데이터에 대한 예측 |\n",
    "| `get_config()` | 모델 저장 및 로드 시 설정 유지 | 모델 직렬화 지원 |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, num_classes=10, dropout_rate=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        사용자 정의 CNN 모델 (Keras Model 상속)\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): 출력 클래스 개수\n",
    "            dropout_rate (float): 드롭아웃 비율\n",
    "            **kwargs: 추가적인 Keras Model 설정\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # 🔹 CNN 기반 신경망 정의\n",
    "        self.conv1 = layers.Conv2D(32, (3, 3), activation='relu', name=\"conv1\")\n",
    "        self.pool1 = layers.MaxPooling2D((2, 2), name=\"pool1\")\n",
    "        self.conv2 = layers.Conv2D(64, (3, 3), activation='relu', name=\"conv2\")\n",
    "        self.pool2 = layers.MaxPooling2D((2, 2), name=\"pool2\")\n",
    "        self.flatten = layers.Flatten(name=\"flatten\")\n",
    "        self.fc1 = layers.Dense(128, activation='relu', name=\"fc1\")\n",
    "        self.dropout = layers.Dropout(self.dropout_rate, name=\"dropout\")\n",
    "        self.fc2 = layers.Dense(num_classes, activation='softmax', name=\"fc2\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        순전파(Forward Propagation) 정의\n",
    "        \"\"\"\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        모델 저장 및 로드 시 필요한 설정 정보 반환\n",
    "        Returns:\n",
    "            dict: 모델의 설정 값 저장 (num_classes, dropout_rate 등)\n",
    "        \"\"\"\n",
    "        config = super().get_config()  # 부모 클래스(Keras Model)의 설정 정보 가져오기\n",
    "        config.update({\n",
    "            \"num_classes\": self.num_classes,  # 클래스 개수 저장\n",
    "            \"dropout_rate\": self.dropout_rate  # 드롭아웃 비율 저장\n",
    "        })\n",
    "        return config  # 직렬화 가능한 딕셔너리 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
